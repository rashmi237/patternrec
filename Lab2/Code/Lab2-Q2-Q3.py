
# coding: utf-8

# # Lab 2:  Model Estimation and Discriminant Functions

# ### Functions and Setup

# In[2]:


import numpy as np
import scipy.stats as stats
import matplotlib.pyplot as plt
from scipy.stats import kde


# In[24]:


def create_exp_pdf(x, lambda_):
    exp_pdf = np.zeros(len(x))
    for i, x_pos in enumerate(x):
        if x_pos>=0:
            exp_pdf[i] = lambda_*np.exp(-lambda_*x_pos)
        else:
            exp_pdf[i] = 0 
    return exp_pdf

def create_uniform_pdf(x,a,b):
    uniform_pdf = np.zeros(len(x))
    for i, x_pos in enumerate(x):
        if (x_pos>=a) and (x_pos<=b):
            uniform_pdf[i] = 1/(b-a)
        else:
            uniform_pdf[i] = 0 
    return uniform_pdf

def parzen(x,samples,sigma):
    p_est = np.zeros(len(x))
    N = len(x)
    #create an estimate PDF (p_est) for every point in x
    for i, x_pos in enumerate(x):
        for s in samples:
            p_est[i] += (1/N) * 1/(np.sqrt(2*np.pi*sigma**2)) * np.exp( (-(x_pos-s)**2) / (2*sigma**2) )
            #stats.norm.pdf(x_pos, mu_est_gaussian, sigma_est_gaussian)
    return p_est


# ## 2 - Model Estimation 1-D case
# 
# ### Part 1 : Parametric Estimation – Gaussian
# 
# From the lecture, we know that the ML estimates for a given assumed univariate normal distribution will be the derivatives of the log likelihood. For mu, the esitimate will be mu_est = 1/N * sum(x_i) for i=1->N. For variance, it will be var_est = 1/N * sum( (x_i - mu_est)^2 ) for i=1->N.
# 
# Note: we still need to convert this sample set i generated here into the one generated by that matlab file

# In[25]:


#get data set a
with open('lab2_1_a.txt','r') as f_open:
    a = f_open.read()
samples_a = [float(s) for s in a.split(' ')]   
N = len(samples_a)

#get data set b
with open('lab2_1_b.txt','r') as f_open:
    b = f_open.read()
samples_b = [float(s) for s in b.split(' ')]   
N = len(samples_b)

mu, sigma = 5, 1
lambda_ = 1

#Generate True PDFs 
#Gaussian
x_a = np.linspace(np.amin(samples_a), np.amax(samples_a), N)
norm_pdf = stats.norm.pdf(x_a, mu, sigma)
#Exponential
x_b = np.linspace(np.amin(samples_b), np.amax(samples_b), N)
exp_pdf = create_exp_pdf(x_b, lambda_)


# In[27]:


#ML classifiers - Gaussian Samples
mu_est_gaussian = 1/N * np.sum(samples_a)
sigma_est_gaussian = np.sqrt(1/N * np.sum((samples_a-mu_est_gaussian)**2))

#ML classifiers - Exponential Samples
mu_est_exp = 1/N * np.sum(samples_b)
sigma_est_exp = np.sqrt(1/N * np.sum((samples_b-mu_est_exp)**2))        
        
#Generate Gaussian Estimated PDFs
norm_pdf_est_gaussian = stats.norm.pdf(x_a, mu_est_gaussian, sigma_est_gaussian) #gaussian samples
norm_pdf_est_exp = stats.norm.pdf(x_b, mu_est_exp, sigma_est_exp) #exponential samples
 
#PLOTs
f, (ax1, ax2) = plt.subplots(1,2, figsize=(16, 6))
ax1.plot(x_a, norm_pdf, label='True Gaussian PDF')
ax1.plot(x_a, norm_pdf_est_gaussian, label='Estimated Gaussian PDF')
ax1.plot(samples_a, np.zeros(len(samples_a)), 'x', color='tab:purple', label='Gaussian Samples')
ax1.axvline(x=mu, alpha = 0.7, ymin=0, ymax=1, linestyle='--')
ax1.axvline(x=mu_est_gaussian, alpha = 0.7, ymin=0, ymax=1, linestyle='--', color='tab:orange')
ax1.set_xlabel('x')
ax1.set_ylabel('p(x) | $\hat{p}(x)$')
ax1.set_title('Gaussian Parametric Estimation - Gaussian Samples')
ax1.legend(loc='best');

ax2.plot(x_b, exp_pdf, label='True Exponential PDF')
ax2.plot(x_b, norm_pdf_est_exp, label='Estimated Gaussian PDF')
ax2.plot(samples_b, np.zeros(len(samples_b)), 'x', color='tab:purple', label='Exponential Samples')
ax2.axvline(x=lambda_, alpha = 0.7, ymin=0, ymax=0.75, linestyle='--')
ax2.axvline(x=mu_est_exp, alpha = 0.7, ymin=0, ymax=0.75, linestyle='--', color='tab:orange')
ax2.set_xlabel('x')
ax2.set_ylabel('p(x) | $\hat{p}(x)$')
ax2.set_title('Gaussian Parametric Estimation - Exponential Samples')
ax2.legend(loc='best');

print("Gaussian Samples: true mean = %s, estimated mean = %s" % (mu, mu_est_gaussian))
print("Gaussian Samples: true variance = %s, estimated variance = %s" % (sigma, sigma_est_gaussian))
print("-------")
print("Exponential Samples: true mean = %s, estimated mean = %s" % (1/lambda_, mu_est_exp))
print("Exponential Samples: true variance = %s, estimated variance = %s" % (1/(lambda_**2), sigma_est_exp))


# ### Part 2 : Parametric Estimation – Exponential
# 
# The maximum likelihood paramater can be estimated by taking the derivative of λ^n * exp(-λ*n*x_est) where x_est = 1/N * sum(x_i) for i=1->N and is the same as in part 1. To maximize, take this derivative and equate to 0. That gives n/λ - nx_est = 0 or λ = 1/x_est = n / sum(x_i) for i=1->N

# In[5]:


#ML classifiers - Gaussian Samples
lambda_est_gaussian = N / np.sum(samples_a)

#ML classifiers - Exponential Samples
lambda_est_exp = N / np.sum(samples_b)
      
        
#Generate Exponentially Estimated PDFs
exp_pdf_est_gaussian = create_exp_pdf(x_a, lambda_est_gaussian)
exp_pdf_est_exp = create_exp_pdf(x_b, lambda_est_exp) 

#PLOTs
f, (ax1, ax2) = plt.subplots(1,2, figsize=(16, 6))
ax1.plot(x_a, norm_pdf, label='True Gaussian PDF')
ax1.plot(x_a, exp_pdf_est_gaussian, label='Estimated Exponential PDF')
ax1.plot(samples_a, np.zeros(len(samples_a)), 'x', color='tab:purple', label='Gaussian Samples')
ax1.axvline(x=1/mu, alpha = 0.7, ymin=0, ymax=1, linestyle='--')
ax1.axvline(x=lambda_est_gaussian, alpha = 0.7, ymin=0, ymax=1, linestyle='--', color='tab:orange')
ax1.set_xlabel('x')
ax1.set_ylabel('p(x) | $\hat{p}(x)$')
ax1.set_title('Exponential Parametric Estimation - Gaussian Samples')
ax1.legend(loc='best');

ax2.plot(x_b, exp_pdf, label='True Exponential PDF')
ax2.plot(x_b, exp_pdf_est_exp, label='Estimated Exponential PDF')
ax2.plot(samples_b, np.zeros(len(samples_b)), 'x', color='tab:purple', label='Exponential Samples')
ax2.axvline(x=lambda_, alpha = 0.7, ymin=0, ymax=0.75, linestyle='--')
ax2.axvline(x=lambda_est_exp, alpha = 0.7, ymin=0, ymax=0.75, linestyle='--', color='tab:orange')
ax2.set_xlabel('x')
ax2.set_ylabel('p(x) | $\hat{p}(x)$')
ax2.set_title('Exponential Parametric Estimation - Exponential Samples')
ax2.legend(loc='best');

print("Gaussian Samples: true lambda = %s, estimated lambda  = %s" % (1/mu, lambda_est_gaussian))
print("-------")
print("Exponential Samples: true lambda = %s, estimated lambda = %s" % (lambda_, lambda_est_exp))


# ### Part 3 : Parametric Estimation – Uniform
# 
# The likelihood function of uniform distribution is the multiplication of all 1/(b-a) from 1 to N. this gives a log likelihood function = -nlog(b-a). the derivative of this wrt a is = n/(b-a). the derivative of this wrt b is = -n/(b-a). 
# 
# Maximizing the first equation would be achieved through making a as large as possible as (b-a)->0. Maximizing the second equation would be achieved through making b as large as possible as (b-a)->0. In both cases however, the MLE is achieved through making (b-a) as small as possible. As such, for any data set [X0 Xn], b will be the largest value in this set and a will be the smallest. 

# In[6]:


#ML classifiers - Gaussian Samples
a_est_gaussian = np.amin(samples_a)
b_est_gaussian = np.amax(samples_a)

#ML classifiers - Exponential Samples
a_est_exp = np.amin(samples_b)
b_est_exp = np.amax(samples_b)

#Generate Exponentially Estimated PDFs
uniform_pdf_est_gaussian = create_uniform_pdf(x_a, a_est_gaussian, b_est_gaussian)
uniform_pdf_est_exp = create_uniform_pdf(x_b, a_est_exp, b_est_exp) 

#PLOTs
f, (ax1, ax2) = plt.subplots(1,2, figsize=(16, 6))
ax1.plot(x_a, norm_pdf, label='True Gaussian PDF')
ax1.plot(x_a, uniform_pdf_est_gaussian, label='Estimated Uniform PDF')
ax1.plot(samples_a, np.zeros(len(samples_a)), 'x', color='tab:purple', label='Gaussian Samples')
ax1.axvline(x=mu, alpha = 0.7, ymin=0, ymax=1, linestyle='--')
ax1.axvline(x=(b_est_gaussian+a_est_gaussian)/2, alpha = 0.7, ymin=0, ymax=1, linestyle='--', color='tab:orange')
ax1.set_xlabel('x')
ax1.set_ylabel('p(x) | $\hat{p}(x)$')
ax1.set_title('Uniform Parametric Estimation - Gaussian Samples')
ax1.legend(loc='best');

ax2.plot(x_b, exp_pdf, label='True Exponential PDF')
ax2.plot(x_b, uniform_pdf_est_exp, label='Estimated Uniform PDF')
ax2.plot(samples_b, np.zeros(len(samples_b)), 'x', color='tab:purple', label='Exponential Samples')
ax2.axvline(x=lambda_, alpha = 0.7, ymin=0, ymax=0.75, linestyle='--')
ax2.axvline(x=(b_est_exp+a_est_exp)/2, alpha = 0.7, ymin=0, ymax=0.75, linestyle='--', color='tab:orange')
ax2.set_xlabel('x')
ax2.set_ylabel('p(x) | $\hat{p}(x)$')
ax2.set_title('Uniform Parametric Estimation - Exponential Samples')
ax2.legend(loc='best');

print("Gaussian Samples: estimated a = %s, estimated b = %s" % (a_est_gaussian, b_est_gaussian))
print("-------")
print("Exponential Samples: estimated a = %s, estimated b = %s" % (a_est_exp, b_est_exp))


# ### Part 4 : Non-Parametric Estimation 
# 
# h is proportional to std_dev

# In[7]:


p_est_gaussian1 = parzen(x_a, samples_a, 0.1)
p_est_gaussian2 = parzen(x_a, samples_a, 0.4)

p_est_exp1 = parzen(x_b, samples_b, 0.1)
p_est_exp2 = parzen(x_b, samples_b, 0.4)

#PLOTs
f, (ax1, ax2) = plt.subplots(1,2, figsize=(16, 6))
ax1.plot(x_a, norm_pdf, label='True Gaussian PDF')
ax1.plot(x_a, p_est_gaussian1, label='Parzen Window Estimation')
ax1.plot(samples_a, np.zeros(len(samples_a)), 'x', color='tab:purple', label='Gaussian Samples')
ax1.set_xlabel('x')
ax1.set_ylabel('p(x) | $\hat{p}(x)$')
ax1.set_title('Non-Parametric Estimation (σ=0.1) - Gaussian Samples')
ax1.legend(loc='best');

ax2.plot(x_a, norm_pdf, label='True Gaussian PDF')
ax2.plot(x_a, p_est_gaussian2, label='Parzen Window Estimation')
ax2.plot(samples_a, np.zeros(len(samples_a)), 'x', color='tab:purple', label='Gaussian Samples')
ax2.set_xlabel('x')
ax2.set_ylabel('p(x) | $\hat{p}(x)$')
ax2.set_title('Non-Parametric Estimation (σ=0.4) - Gaussian Samples')
ax2.legend(loc='best');

#PLOTs
f, (ax1, ax2) = plt.subplots(1,2, figsize=(16, 6))
ax1.plot(x_b, exp_pdf, label='True Exponential PDF')
ax1.plot(x_b, p_est_exp1, label='Parzen Window Estimation')
ax1.plot(samples_b, np.zeros(len(samples_b)), 'x', color='tab:purple', label='Exponential Samples')
ax1.set_xlabel('x')
ax1.set_ylabel('p(x) | $\hat{p}(x)$')
ax1.set_title('Non-Parametric Estimation (σ=0.1) - Exponential Samples')
ax1.legend(loc='best');

ax2.plot(x_b, exp_pdf, label='True Exponential PDF')
ax2.plot(x_b, p_est_exp2, label='Parzen Window Estimation')
ax2.plot(samples_b, np.zeros(len(samples_b)), 'x', color='tab:purple', label='Exponential Samples')
ax2.set_xlabel('x')
ax2.set_ylabel('p(x) | $\hat{p}(x)$')
ax2.set_title('Non-Parametric Estimation (σ=0.4) - Exponential Samples')
ax2.legend(loc='best');


# ## 3  Model Estimation 2-D case
# 
# ### Part 1 : Parametric estimation

# In[4]:


#get data set al
data_al = []
with open('lab2_2_al.txt','r') as f:
    for line in f.readlines():
        if(line.rstrip()):
            line = line[1:]
            data_al.append([float(s) for s in line.split(' ')])
#easier to deal with NumPy arrays
samples_al = np.asarray(data_al)

#get data set bl
data_bl = []
with open('lab2_2_bl.txt','r') as f:
    for line in f.readlines():
        if(line.rstrip()):
            line = line[1:]
            data_bl.append([float(s) for s in line.split(' ')])
samples_bl = np.asarray(data_bl)


#get data set cl
data_cl = []
with open('lab2_2_cl.txt','r') as f:
    for line in f.readlines():
        if(line.rstrip()):
            line = line[1:]
            data_cl.append([float(s) for s in line.split(' ')])
samples_cl = np.asarray(data_cl)



# In[6]:



n_al = len(samples_al)
n_bl = len(samples_bl)
n_cl = len(samples_cl)

# calculate the sample mean
mu_est_al = sum(samples_al)/n_al
mu_est_bl = sum(samples_bl)/n_bl
mu_est_cl = sum(samples_cl)/n_cl

print(mu_est_al)

print(mu_est_bl)

print(mu_est_cl)


# calculate the sample covariance 

res1 = np.matmul(np.transpose(samples_al - mu_est_al),(samples_al - mu_est_al))
cov_est_al = res1/len(samples_al)

res2 = np.matmul(np.transpose(samples_bl - mu_est_bl),(samples_bl - mu_est_bl))
cov_est_bl = res2/len(samples_bl)

res3 = np.matmul(np.transpose(samples_cl - mu_est_cl),(samples_cl - mu_est_cl))
cov_est_cl = res3/len(samples_cl)

# print(cov_est_cl)
# print(np.cov(np.transpose(samples_bl))) 

print(cov_est_al)

print(cov_est_bl)

print(cov_est_cl)


# In[7]:


# # create test samples 
# al_test = np.random.multivariate_normal(mu_est_al, cov_est_al, n_al)
# bl_test = np.random.multivariate_normal(mu_est_bl, cov_est_bl, n_bl)
# cl_test = np.random.multivariate_normal(mu_est_cl, cov_est_cl, n_cl)

# print(al_test)

#create x,y grid for 2D case
X2 = np.concatenate((samples_al,samples_bl,samples_cl), axis = 0)
h2 = .5
x2_min, x2_max = X2[:, 0].min() - 1, X2[:, 0].max() + 1
y2_min, y2_max = X2[:, 1].min() - 1, X2[:, 1].max() + 1

xx2, yy2 = np.meshgrid(np.arange(x2_min, x2_max, h2),
                     np.arange(y2_min, y2_max, h2))

# print(X2)


# In[32]:


# Using our MAP classifier from lab1 but with P(A) = P(B) so that it can be used as an ML classifier 

def ML_val(classifier, mu_a, mu_b, std_a, std_b, X, Y):
    
    pb = np.matmul(mu_b,np.linalg.inv(std_b))
    pa = np.matmul(mu_a,np.linalg.inv(std_a))
    Q0 = np.linalg.inv(std_a) - np.linalg.inv(std_b)
    Q1 = 2*(pb-pa)
    Q2 = np.matmul(pa,np.transpose(mu_a))- np.matmul(pb,np.transpose(mu_b))
#     Q3 = np.log(num_b/num_a)
    Q4 = np.log(np.linalg.det(std_a)/np.linalg.det(std_b))
    

    p = np.matmul(classifier,Q0)
    val = np.matmul(p,np.transpose(classifier)) + np.matmul(Q1,np.transpose(classifier)) + Q2 + Q4
            
    return val


# In[33]:


def ML_decision(X,Y):
#     MAP_1 = np.zeros((len(X), len(X[0])), dtype=np.int)
#     MAP_2 = np.zeros((len(X), len(X[0])), dtype=np.int)
#     MAP_3 = np.zeros((len(X), len(X[0])), dtype=np.int)
    dist = np.zeros((len(X), len(X[0])), dtype=np.float)
#     print(cov_a)
#     print(cov_b)
    for i in range(len(X)):
        for j in range(len(X[0])):
            cf = [X[i,j], Y[i,j]]
            
            ML_1 = ML_val(cf, mu_est_al, mu_est_bl, cov_est_al, cov_est_bl, X, Y) # MAP for classes C & D
            ML_2 = ML_val(cf, mu_est_bl, mu_est_cl, cov_est_bl, cov_est_cl, X, Y) # MAP for classes D & E
            ML_3 = ML_val(cf, mu_est_cl, mu_est_al, cov_est_cl, cov_est_al, X, Y) # MAP for classes E & C
            
            if ML_1 < 0 and ML_3 > 0: # CLASS C 
                dist[i,j] = -1;
            elif ML_2 < 0 and ML_1 > 0: # CLASS D
                dist[i,j] = 2;
            elif ML_3 < 0 and ML_2 > 0: # CLASS E 
                dist[i,j] = 3;
                 
    return dist

ML_val2 = ML_decision(xx2,yy2);


# In[34]:




##---------Plot the classes for case 2---------##
plt.figure(1, figsize=(8,8))

plt.contourf(xx2, yy2, ML_val2, colors=('cornflowerblue','tab:purple', 'tab:red' ), edgecolors='face', linewidths='10', alpha=0.5)

plt.scatter(samples_al[:, 0], samples_al[:, 1], marker='x');
plt.scatter(samples_bl[:, 0], samples_bl[:, 1], c='tab:red', marker='$o$')
plt.scatter(samples_cl[:, 0], samples_cl[:, 1], c='tab:purple', marker='X')

plt.plot(mu_est_al[0], mu_est_al[1], c='limegreen', marker='H', markersize=8, alpha=0.5)
plt.plot(mu_est_bl[0], mu_est_bl[1], c='limegreen', marker='H', markersize=8, alpha=0.5)
plt.plot(mu_est_cl[0], mu_est_cl[1], c='limegreen', marker='H', markersize=8, alpha=0.5)

plt.title("Classes AL, BL and CL with Estimated Means")
plt.xlabel('$x$')
plt.ylabel('$y$')
plt.xlim([x2_min, x2_max])
plt.ylim([y2_min, y2_max]);


# ### Part 2 : Non Parametric estimation

# In[8]:


from scipy.stats import multivariate_normal
def parzen2D(x,y,samples,sigma):
    p_est = np.zeros((len(x), len(x[0])), dtype=np.float)
    #create an estimate PDF (p_est) for every point in x
    for i in range(len(x)):
        for j in range(len(x[0])):
            point = [x[i,j], y[i,j]]
            for s in samples:
                p_est[i][j] += multivariate_normal.pdf(point, mean=s, cov=sigma)
    return p_est

def decision(X, Y, ka, kb, kc):
    dist = np.zeros((len(X), len(X[0])), dtype=np.float)
    
    for i in range(len(X)):
        for j in range(len(X[0])):
            cf = [X[i,j], Y[i,j]]
            
            da = ka[i,j]
            db = kb[i,j]
            dc = kc[i,j]

            values = [da,db,dc]
            
            cls = values.index(max(values))
            
            if cls == 0:
                dist[i,j] = -1
            elif cls == 1:
                dist[i,j] = 2
            elif cls == 2:
                dist[i,j] = 3
    return dist


# In[9]:


cov = [[400, 0], [0,400]]
X2 = np.concatenate((samples_al,samples_bl,samples_cl), axis = 0)
h2 = 3
x2_min, x2_max = X2[:, 0].min() - 1, X2[:, 0].max() + 1
y2_min, y2_max = X2[:, 1].min() - 1, X2[:, 1].max() + 1

xx2, yy2 = np.meshgrid(np.arange(x2_min, x2_max, h2),
                     np.arange(y2_min, y2_max, h2))
parz_al= parzen2D(xx2,yy2,samples_al,cov)


# In[10]:


parz_bl= parzen2D(xx2,yy2,samples_bl,cov)


# In[11]:


parz_cl= parzen2D(xx2,yy2,samples_cl,cov)


# In[12]:


from mpl_toolkits.mplot3d import Axes3D
plt.figure(figsize=(10, 8))
ax = plt.axes(projection='3d')
ax.set_xlabel('$dim_1$')
ax.set_ylabel('$dim_2$')
ax.set_zlabel('PDF')
ax.set_title('Estimated PDFs using Gaussian Parzen Windows (σ^2=400)')
ax.title.set_position([.5,1])
ax.plot_surface(xx2,yy2,parz_al,label='Estimated PDF for Cluster A')
ax.plot_surface(xx2,yy2,parz_bl,label='Estimated PDF for Cluster B')
ax.plot_surface(xx2,yy2,parz_cl,label='Estimated PDF for Cluster C');
#ax.plot3D(xs=t, ys=what.T[0], zs=what.T[1], label='$\hat{w}$')
#ax.plot3D(xs=t, ys=ideal_output_w.T[0], zs=ideal_output_w.T[1], label='$w$')


# In[13]:


parzenval = decision(xx2,yy2,parz_al,parz_bl,parz_cl)


# In[14]:


n_al = len(samples_al)
n_bl = len(samples_bl)
n_cl = len(samples_cl)
mu_est_al = sum(samples_al)/n_al
mu_est_bl = sum(samples_bl)/n_bl
mu_est_cl = sum(samples_cl)/n_cl

##---------Plot the classes for case 2---------##
plt.figure(1, figsize=(8,8))
plt.contourf(xx2, yy2, parzenval, colors=('cornflowerblue','tab:purple', 'tab:red' ), edgecolors='face', linewidths='10', alpha=0.5)

plt.scatter(samples_al[:, 0], samples_al[:, 1], marker='x');
plt.scatter(samples_bl[:, 0], samples_bl[:, 1], c='tab:red', marker='$o$')
plt.scatter(samples_cl[:, 0], samples_cl[:, 1], c='tab:purple', marker='X')

plt.plot(mu_est_al[0], mu_est_al[1], c='limegreen', marker='H', markersize=8, alpha=0.5)
plt.plot(mu_est_bl[0], mu_est_bl[1], c='limegreen', marker='H', markersize=8, alpha=0.5)
plt.plot(mu_est_cl[0], mu_est_cl[1], c='limegreen', marker='H', markersize=8, alpha=0.5)

plt.title("Classes AL, BL and CL with Estimated Means")
plt.xlabel('$x$')
plt.ylabel('$y$')
plt.xlim([x2_min, x2_max])
plt.ylim([y2_min, y2_max]);

